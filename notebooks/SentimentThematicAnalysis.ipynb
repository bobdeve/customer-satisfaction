{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment_label  \\\n",
      "0  \"Why don‚Äôt your ATMs support account-to-accoun...        negative   \n",
      "1                        what is this app problem???        negative   \n",
      "2       the app is proactive and a good connections.        positive   \n",
      "3    I cannot send to cbebirr app. through this app.        negative   \n",
      "4                                               good        positive   \n",
      "\n",
      "   sentiment_score  \n",
      "0         0.996465  \n",
      "1         0.999623  \n",
      "2         0.999868  \n",
      "3         0.995335  \n",
      "4         0.999816  \n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "# Load your cleaned CSV\n",
    "# Step 2: Load your cleaned reviews dataset\n",
    "# Replace 'cleaned_reviews.csv' with your actual file name if it's different\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('../data/raw/banks_review_cleaned.csv')\n",
    "# Display the first few rows to understand the structure\n",
    "df.head(50)\n",
    "\n",
    "\n",
    "# Step 3: Load the pre-trained sentiment analysis pipeline from HuggingFace Transformers\n",
    "# We're using a lightweight BERT model fine-tuned on SST-2 dataset for binary sentiment (positive/negative)\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "# Step 4: Prepare review text\n",
    "# Ensure the review column is in string format and truncate long reviews to the first 512 characters\n",
    "# (Transformer models like BERT have a max token limit, usually around 512 tokens)\n",
    "df['short_review'] = df['review'].astype(str).str[:512]\n",
    "\n",
    "# Step 5: Apply the sentiment analysis pipeline to each review\n",
    "# This will return a dictionary with 'label' (POSITIVE or NEGATIVE) and 'score' (confidence)\n",
    "df['sentiment_result'] = df['short_review'].apply(lambda x: sentiment_pipeline(x)[0])\n",
    "\n",
    "# Step 6: Extract 'label' and 'score' from the result dictionary into separate columns\n",
    "df['sentiment_label'] = df['sentiment_result'].apply(lambda x: x['label'])  # POSITIVE or NEGATIVE\n",
    "df['sentiment_score'] = df['sentiment_result'].apply(lambda x: x['score'])  # Confidence score\n",
    "\n",
    "# Optional: Convert label to lowercase for consistency\n",
    "df['sentiment_label'] = df['sentiment_label'].str.lower()\n",
    "\n",
    "# Step 7: Preview the result\n",
    "print(df[['review', 'sentiment_label', 'sentiment_score']].head())\n",
    "\n",
    "# Step 8: Save the result to a new CSV for later use (e.g. thematic analysis)\n",
    "df.to_csv(\"../data/processed/bank_reviews_with_sentiment.csv\", index=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_csv('../data/raw/banks_review_cleaned.csv')\n",
    "# Display the first few rows to understand the structure\n",
    "df.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      bank  rating  mean_sentiment_score  mean_sentiment_label_numeric  \\\n",
      "0      BOA       1              0.986962                     -0.773585   \n",
      "1      BOA       2              0.933640                     -0.750000   \n",
      "2      BOA       3              0.976630                     -0.085714   \n",
      "3      BOA       4              0.964666                      0.000000   \n",
      "4      BOA       5              0.971614                      0.584541   \n",
      "5      CBE       1              0.976445                     -0.622642   \n",
      "6      CBE       2              0.997675                     -0.294118   \n",
      "7      CBE       3              0.986456                     -0.310345   \n",
      "8      CBE       4              0.963908                     -0.043478   \n",
      "9      CBE       5              0.986866                      0.794286   \n",
      "10  Dashen       1              0.995235                     -0.882353   \n",
      "11  Dashen       2              0.981517                     -0.647059   \n",
      "12  Dashen       3              0.997640                      0.000000   \n",
      "13  Dashen       4              0.994449                      0.333333   \n",
      "14  Dashen       5              0.991516                      0.837535   \n",
      "\n",
      "    percent_positive  review_count  \n",
      "0          11.320755           211  \n",
      "1          12.500000            16  \n",
      "2          45.714286            34  \n",
      "3          50.000000            18  \n",
      "4          79.227053           207  \n",
      "5          18.867925            53  \n",
      "6          35.294118            17  \n",
      "7          34.482759            29  \n",
      "8          47.826087            46  \n",
      "9          89.714286           350  \n",
      "10          5.882353            34  \n",
      "11         17.647059            17  \n",
      "12         50.000000            12  \n",
      "13         66.666667            24  \n",
      "14         91.876751           357  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv('../data/raw/banks_review_cleaned.csv')\n",
    "# Display the first few rows to understand the structure\n",
    "df.head(10)\n",
    "\n",
    "df['short_review'] = df['review'].astype(str).str[:512]\n",
    "# Step 6 (continued): Extract the sentiment label and score from the output dictionary\n",
    "df['sentiment_result'] = df['short_review'].apply(lambda x: sentiment_pipeline(x)[0])\n",
    "# Create a new column 'sentiment_label' with the value: POSITIVE or NEGATIVE\n",
    "df['sentiment_label'] = df['sentiment_result'].apply(lambda x: x['label'])\n",
    "\n",
    "# Create a new column 'sentiment_score' which is the model's confidence in its prediction (between 0 and 1)\n",
    "df['sentiment_score'] = df['sentiment_result'].apply(lambda x: x['score'])\n",
    "\n",
    "# Optional: Map sentiment labels to numerical values for easier aggregation\n",
    "# POSITIVE -> 1, NEGATIVE -> -1 (you can also use 0 and 1 if preferred)\n",
    "df['sentiment_numeric'] = df['sentiment_label'].map({'POSITIVE': 1, 'NEGATIVE': -1})\n",
    "\n",
    "# Step 7: Group and aggregate sentiment by bank and star rating\n",
    "# We calculate:\n",
    "# - The average sentiment score\n",
    "# - The proportion of positive reviews\n",
    "# - Total number of reviews in that group (for context)\n",
    "\n",
    "sentiment_summary = df.groupby(['bank', 'rating']).agg(\n",
    "    mean_sentiment_score=('sentiment_score', 'mean'),\n",
    "    mean_sentiment_label_numeric=('sentiment_numeric', 'mean'),\n",
    "    percent_positive=('sentiment_label', lambda x: (x == 'POSITIVE').mean() * 100),\n",
    "    review_count=('review', 'count')\n",
    ").reset_index()\n",
    "\n",
    "# Display the summary\n",
    "print(sentiment_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Top Thematic Keywords from Reviews:\n",
      "               term     score\n",
      "385            good  0.112085\n",
      "41              app  0.073632\n",
      "165            best  0.050803\n",
      "644            nice  0.038064\n",
      "139            bank  0.024281\n",
      "166        best app  0.020417\n",
      "658              ok  0.019762\n",
      "987             wow  0.018982\n",
      "308       excellent  0.017222\n",
      "515            like  0.016630\n",
      "931             use  0.016626\n",
      "386        good app  0.016345\n",
      "157         banking  0.016117\n",
      "403           great  0.015948\n",
      "974            work  0.015242\n",
      "325            fast  0.014396\n",
      "118     application  0.014150\n",
      "291            easy  0.013905\n",
      "240          dashen  0.013863\n",
      "22          amazing  0.013760\n",
      "976         working  0.012112\n",
      "862           super  0.011800\n",
      "196             cbe  0.011305\n",
      "598          mobile  0.011165\n",
      "182             boa  0.010922\n",
      "981           worst  0.009053\n",
      "889           thank  0.008861\n",
      "280           doesn  0.008262\n",
      "601  mobile banking  0.008258\n",
      "241     dashen bank  0.008142\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Use short version of the review or full version\n",
    "texts = df['short_review'].fillna(\"\")\n",
    "\n",
    "# Step 2: Create a TF-IDF Vectorizer\n",
    "# We use unigrams, bigrams, and trigrams (1 to 3-word phrases)\n",
    "# This helps extract phrases like \"login error\", \"transfer failed\", \"slow app\"\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,3), \n",
    "                             stop_words='english',  # remove common words\n",
    "                             max_features=1000)     # limit to top 1000 keywords\n",
    "\n",
    "# Step 3: Fit the vectorizer and transform the reviews into TF-IDF matrix\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 4: Get top n keywords by average TF-IDF score across all reviews\n",
    "import numpy as np\n",
    "\n",
    "# Compute mean tf-idf score for each feature (keyword/ngram)\n",
    "mean_tfidf = np.asarray(tfidf_matrix.mean(axis=0)).flatten()\n",
    "\n",
    "# Map scores to terms\n",
    "tfidf_scores = pd.DataFrame({\n",
    "    'term': vectorizer.get_feature_names_out(),\n",
    "    'score': mean_tfidf\n",
    "})\n",
    "\n",
    "# Sort to get top themes\n",
    "top_keywords = tfidf_scores.sort_values(by='score', ascending=False).head(30)\n",
    "print(\"üîç Top Thematic Keywords from Reviews:\")\n",
    "print(top_keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: good, app, work, ok, better, boa, make, working, update, don\n",
      "Topic #2: app, super, dashen, fast, super app, user, easy, dashen bank, features, banking\n",
      "Topic #3: app, best, best app, good, good app, like, use, banking, bank, worst\n",
      "Topic #4: app, bank, nice, cbe, wow, great, dashen, step, developer, dashen bank\n",
      "Topic #5: app, mobile, banking, mobile banking, work, doesn, application, amazing, doesn work, excellent\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Step 1: Prepare review text (same as before)\n",
    "texts = df['short_review'].fillna(\"\")\n",
    "\n",
    "# Step 2: Convert text into a document-term matrix\n",
    "# Using unigrams and bigrams to capture phrases like \"login error\"\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 2), max_features=1000)\n",
    "dtm = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Step 3: Apply LDA to discover topics\n",
    "n_topics = 5  # You can tweak this based on results\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "lda.fit(dtm)\n",
    "\n",
    "# Step 4: Print top keywords in each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "def get_top_keywords(model, feature_names, n_top_words):\n",
    "    topics = []\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_keywords = [feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]\n",
    "        topics.append(top_keywords)\n",
    "        print(f\"Topic #{topic_idx + 1}: {', '.join(top_keywords)}\")\n",
    "    return topics\n",
    "\n",
    "# Get top 10 words per topic\n",
    "topics = get_top_keywords(lda, feature_names, n_top_words=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "\n",
    "# Load English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Sample reviews\n",
    "texts = df['short_review'].fillna(\"\")\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess(text):\n",
    "    doc = nlp(text.lower())\n",
    "    tokens = [token.lemma_ for token in doc if token.is_alpha and not token.is_stop]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "df['cleaned_review'] = texts.apply(preprocess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "sentiment_pipeline = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "df['sentiment'] = df['short_review'].apply(lambda x: sentiment_pipeline(x)[0]['label'] if isinstance(x, str) else None)\n",
    "df['sentiment_score'] = df['short_review'].apply(lambda x: sentiment_pipeline(x)[0]['score'] if isinstance(x, str) else None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Extract top keywords\n",
    "vectorizer = TfidfVectorizer(max_features=20, ngram_range=(1,2))\n",
    "X = vectorizer.fit_transform(df['cleaned_review'])\n",
    "\n",
    "# Add top keywords to DataFrame\n",
    "keywords = vectorizer.get_feature_names_out()\n",
    "tfidf_matrix = pd.DataFrame(X.toarray(), columns=keywords)\n",
    "df_keywords = pd.concat([df, tfidf_matrix], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example mapping logic based on top LDA keywords\n",
    "def map_to_theme(text):\n",
    "    text = text.lower()\n",
    "    if \"login\" in text or \"access\" in text or \"otp\" in text:\n",
    "        return \"Account Access Issues\"\n",
    "    elif \"slow\" in text or \"transfer\" in text or \"transaction\" in text:\n",
    "        return \"Transaction Performance\"\n",
    "    elif \"ui\" in text or \"interface\" in text or \"design\" in text:\n",
    "        return \"User Interface & Experience\"\n",
    "    elif \"support\" in text or \"help\" in text or \"response\" in text:\n",
    "        return \"Customer Support\"\n",
    "    elif \"feature\" in text or \"request\" in text:\n",
    "        return \"Feature Request\"\n",
    "    else:\n",
    "        return \"Other\"\n",
    "\n",
    "df['theme'] = df['cleaned_review'].apply(map_to_theme)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df[['short_review', 'cleaned_review', 'sentiment', 'sentiment_score', 'theme', 'bank', 'rating', 'date', 'source']]\n",
    "df_final.to_csv(\"../data/processed/bank_review_sentiment_themes.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webscraper",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
