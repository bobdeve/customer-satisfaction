{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google_play_scraper import reviews, Sort\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“¥ Scraping reviews for CBE...\n",
      "ðŸ“¥ Scraping reviews for BOA...\n",
      "ðŸ“¥ Scraping reviews for Dashen...\n",
      "\n",
      "âœ… Done! Scraped and cleaned 1382 reviews across 3 banks.\n",
      "ðŸ’¾ Data saved to: data\\raw\\bank_reviews_cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "from google_play_scraper import reviews, Sort\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# ----------------------------\n",
    "# 1. Define the target apps with package names and bank names\n",
    "# ----------------------------\n",
    "# This dictionary maps each app's unique package identifier on Google Play\n",
    "# to a human-friendly bank name to label our data\n",
    "apps = {\n",
    "    'com.combanketh.mobilebanking': 'CBE',       # Commercial Bank of Ethiopia\n",
    "    'com.boa.boaMobileBanking': 'BOA',           # Bank of Abyssinia\n",
    "    'com.dashen.dashensuperapp': 'Dashen'        # Dashen Bank\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Create folder(s) to store scraped data if they don't exist\n",
    "# ----------------------------\n",
    "# We create a nested folder path 'data/raw' where the final CSV will be saved\n",
    "os.makedirs('data/raw', exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 3. Initialize a list to hold DataFrames for each bank's reviews\n",
    "# ----------------------------\n",
    "# We will collect reviews from each bank separately and store them temporarily here\n",
    "all_reviews = []\n",
    "\n",
    "# ----------------------------\n",
    "# 4. Loop through each app to scrape reviews\n",
    "# ----------------------------\n",
    "for package, bank_name in apps.items():\n",
    "    print(f\"ðŸ“¥ Scraping reviews for {bank_name}...\")\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4a. Use google_play_scraper to fetch reviews\n",
    "    # ----------------------------\n",
    "    # Fetch up to 600 recent reviews to account for possible data cleaning losses\n",
    "    # 'lang' and 'country' set to English reviews from the US store (adjust if needed)\n",
    "    # 'Sort.NEWEST' fetches the most recent reviews first\n",
    "    bank_reviews, _ = reviews(\n",
    "        package,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=Sort.NEWEST,\n",
    "        count=600   # Fetch more than 400 to have enough valid reviews after cleaning\n",
    "    )\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4b. Convert scraped reviews to a pandas DataFrame\n",
    "    # ----------------------------\n",
    "    # Extract only relevant columns: review text, rating score, and date of review\n",
    "    df = pd.DataFrame(bank_reviews)[['content', 'score', 'at']]\n",
    "\n",
    "    # Rename columns for clarity and consistency\n",
    "    df.columns = ['review', 'rating', 'date']\n",
    "\n",
    "    # Add columns for metadata: which bank the review belongs to and data source\n",
    "    df['bank'] = bank_name\n",
    "    df['source'] = 'Google Play'\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4c. Data Cleaning: Remove duplicates and missing data\n",
    "    # ----------------------------\n",
    "    # Drop duplicate reviews based on the review text to avoid counting repeats\n",
    "    df.drop_duplicates(subset='review', inplace=True)\n",
    "\n",
    "    # Drop any rows where critical data is missing (review, rating, or date)\n",
    "    df.dropna(subset=['review', 'rating', 'date'], inplace=True)\n",
    "\n",
    "    # ----------------------------\n",
    "    # 4d. Validate review count after cleaning\n",
    "    # ----------------------------\n",
    "    # Check if after cleaning, we have fewer than 400 valid reviews for this bank\n",
    "    if len(df) < 400:\n",
    "        print(f\"âš ï¸ Warning: Only {len(df)} valid reviews for {bank_name}, less than 400.\")\n",
    "\n",
    "    # Add this cleaned DataFrame to our list for later concatenation\n",
    "    all_reviews.append(df)\n",
    "\n",
    "# ----------------------------\n",
    "# 5. Combine reviews from all banks into one DataFrame\n",
    "# ----------------------------\n",
    "df_all = pd.concat(all_reviews, ignore_index=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 6. Normalize the 'date' column to a consistent string format (YYYY-MM-DD)\n",
    "# ----------------------------\n",
    "df_all['date'] = pd.to_datetime(df_all['date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# ----------------------------\n",
    "# 7. Save the cleaned combined data to a CSV file\n",
    "# ----------------------------\n",
    "# Path to save the CSV file inside the 'data/raw' directory\n",
    "csv_path = os.path.join('data', 'raw', 'bank_reviews_cleaned.csv')\n",
    "\n",
    "# Save DataFrame to CSV without the DataFrame index column\n",
    "df_all.to_csv(csv_path, index=False)\n",
    "\n",
    "# ----------------------------\n",
    "# 8. Final status message\n",
    "# ----------------------------\n",
    "print(f\"\\nâœ… Done! Scraped and cleaned {len(df_all)} reviews across 3 banks.\")\n",
    "print(f\"ðŸ’¾ Data saved to: {csv_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
